{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyD5OvyUAtzA2Cdy-s55xDRYwcJOZO4exJo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       video_id                                              title  \\\n",
      "0   oKTUw78thcg  Basic Avalanche Theory &amp; Identifying Avala...   \n",
      "1   -luspmL_f1o                Avalanche Safety and Risk Awareness   \n",
      "2   3dV86N35h5s                               Avalanche Mitigation   \n",
      "3   ZFWII5bAlQI  Avalanche types and their trigger mechanisms â€“...   \n",
      "4   X90kUtxMd_Q  Avalanche Rescue: How to Use Your Beacon, Prob...   \n",
      "..          ...                                                ...   \n",
      "95  qwmucsjJJRc  What Avalanche Safety Equipment Is Essential? ...   \n",
      "96  G7j6aQp3iiM  Backcountry Skiing Safety with The Utah Avalan...   \n",
      "97  SQS55aOVMxA  Avalanche Risk! New Study on Heated Gloves Wil...   \n",
      "98  vfImpXCppso  What Are The Best Avalanche Safety Courses? - ...   \n",
      "99  mkFnTMduEk8  An operational perspective on avalanche safety...   \n",
      "\n",
      "                                          description             published  \n",
      "0   We've teamed up with Altus Mountain Guides to ...  2020-12-17T08:08:11Z  \n",
      "1   On April 20, 2024, we lost a friend. Rob Coppo...  2021-03-03T12:00:46Z  \n",
      "2                                                      2024-01-26T15:15:27Z  \n",
      "3   What types of avalanche are there and how are ...  2022-11-28T08:01:13Z  \n",
      "4   In the case of an avalanche, it's critical to ...  2020-09-08T17:13:48Z  \n",
      "..                                                ...                   ...  \n",
      "95  What Avalanche Safety Equipment Is Essential? ...  2025-03-15T02:13:59Z  \n",
      "96  Every backcountry accident comes down to a cho...  2022-12-01T21:46:39Z  \n",
      "97  Skiers, climbers, and snowmobilers all concern...  2023-10-19T11:15:01Z  \n",
      "98  What Are The Best Avalanche Safety Courses? Ar...  2025-03-24T13:17:58Z  \n",
      "99  12/4/21 Presenter: Neil Satterfield, Mammoth M...  2021-12-14T02:44:17Z  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Function to get video metadata in batches of 50\n",
    "def get_videos(query, max_results=100):\n",
    "    videos = []\n",
    "    for i in range(0, max_results, 50):  # Loop in batches of 50 videos\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            type=\"video\",\n",
    "            maxResults=50,\n",
    "            pageToken=None if i == 0 else next_page_token  # Handling pagination\n",
    "        )\n",
    "        response = request.execute()\n",
    "        next_page_token = response.get('nextPageToken', None)\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"][\"videoId\"]\n",
    "            title = item[\"snippet\"][\"title\"]\n",
    "            description = item[\"snippet\"][\"description\"]\n",
    "            published = item[\"snippet\"][\"publishedAt\"]\n",
    "\n",
    "            videos.append({\"video_id\": video_id, \"title\": title, \"description\": description, \"published\": published})\n",
    "\n",
    "    return pd.DataFrame(videos)\n",
    "\n",
    "# Example: Search for \"avalanche safety\" videos and retrieve metadata for 100 videos\n",
    "df_videos = get_videos(\"avalanche safety\", max_results=100)\n",
    "print(df_videos)\n",
    "\n",
    "df_videos.to_csv('/Users/terezasaskova/Desktop/youtubeAPI/youtube_api_videos_info.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       video_id     views   likes comments\n",
      "0   oKTUw78thcg     78858     899       36\n",
      "1   -luspmL_f1o     40508     633       20\n",
      "2   3dV86N35h5s  35663975  982974     2669\n",
      "3   ZFWII5bAlQI     32670       0        1\n",
      "4   X90kUtxMd_Q     49800     678       24\n",
      "..          ...       ...     ...      ...\n",
      "95  qwmucsjJJRc         0       0        0\n",
      "96  G7j6aQp3iiM      1992      29        4\n",
      "97  SQS55aOVMxA      1279      32        0\n",
      "98  vfImpXCppso         0       0        0\n",
      "99  mkFnTMduEk8       139       2        0\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_video_stats(video_ids):\n",
    "    stats = []\n",
    "    for i in range(0, len(video_ids), 50):  # Loop in batches of 50 videos\n",
    "        batch_video_ids = video_ids[i:i + 50]\n",
    "        request = youtube.videos().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(batch_video_ids)\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            video_id = item[\"id\"]\n",
    "            views = item[\"statistics\"].get(\"viewCount\", 0)\n",
    "            likes = item[\"statistics\"].get(\"likeCount\", 0)\n",
    "            comments = item[\"statistics\"].get(\"commentCount\", 0)\n",
    "\n",
    "            stats.append({\"video_id\": video_id, \"views\": views, \"likes\": likes, \"comments\": comments})\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# Example: Fetch stats for first 100 videos (from df_videos)\n",
    "video_ids = df_videos[\"video_id\"].tolist()\n",
    "df_stats = get_video_stats(video_ids)\n",
    "print(df_stats)\n",
    "df_stats.to_csv('/Users/terezasaskova/Desktop/youtubeAPI/video_stats.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments for video 4igL_e-a4uM: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=4igL_e-a4uM&maxResults=100&textFormat=plainText&key=AIzaSyD5OvyUAtzA2Cdy-s55xDRYwcJOZO4exJo&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "         video_id                 author  \\\n",
      "0     oKTUw78thcg   @dhandymandarren3074   \n",
      "1     oKTUw78thcg          @eloncarr7372   \n",
      "2     oKTUw78thcg  @blanchedevereaux5403   \n",
      "3     oKTUw78thcg          @LuisBreniser   \n",
      "4     oKTUw78thcg     @maxwellrivers3714   \n",
      "...           ...                    ...   \n",
      "2267  UBlAsJLy2wM           @jazzzyj7589   \n",
      "2268  G7j6aQp3iiM        @Andrew.Drennan   \n",
      "2269  G7j6aQp3iiM                @at1970   \n",
      "2270  G7j6aQp3iiM           @jahlight541   \n",
      "2271  G7j6aQp3iiM            @WNDRAlpine   \n",
      "\n",
      "                                                   text  likes  \\\n",
      "0     Moral is don't ski or board on steep slopes wh...      0   \n",
      "1     Thank you for publishing this! I need advice: ...      0   \n",
      "2     This might be a dumb question, but how do skie...      0   \n",
      "3     Thanks for posting! I need advice: My Safe Wal...      0   \n",
      "4     Cheers for posting! Hoping for some guidance: ...      0   \n",
      "...                                                 ...    ...   \n",
      "2267  This was some amazing filming and hits close t...      0   \n",
      "2268                              The man bun of truth!      1   \n",
      "2269  The most amazing thing? You can actually find ...      0   \n",
      "2270  Absolutely!  I've been gratefully following th...      0   \n",
      "2271  Best COMMUNITY on Earth! Thanks for keeping us...      0   \n",
      "\n",
      "                 published  \n",
      "0     2025-03-28T10:09:28Z  \n",
      "1     2025-03-13T04:52:45Z  \n",
      "2     2025-03-07T22:19:06Z  \n",
      "3     2025-03-03T22:56:11Z  \n",
      "4     2025-02-27T00:55:09Z  \n",
      "...                    ...  \n",
      "2267  2021-02-19T18:40:54Z  \n",
      "2268  2022-12-29T20:09:24Z  \n",
      "2269  2022-12-08T23:32:42Z  \n",
      "2270  2022-12-08T22:45:43Z  \n",
      "2271  2022-12-06T23:10:38Z  \n",
      "\n",
      "[2272 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_video_comments(video_id, max_results=100):\n",
    "    comments = []\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=max_results,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "            text = comment[\"textDisplay\"]\n",
    "            author = comment[\"authorDisplayName\"]\n",
    "            likes = comment[\"likeCount\"]\n",
    "            published = comment[\"publishedAt\"]\n",
    "\n",
    "            comments.append({\"video_id\": video_id, \"author\": author, \"text\": text, \"likes\": likes, \"published\": published})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "        # Optionally, log or track the videos where comments are disabled\n",
    "        return None\n",
    "\n",
    "    return pd.DataFrame(comments) if comments else None\n",
    "\n",
    "# Fetch comments for all videos, skipping those with disabled comments\n",
    "df_comments_all = pd.DataFrame()  # Empty DataFrame to append comments\n",
    "\n",
    "for video_id in video_ids[:100]:  # Loop through first 100 video IDs\n",
    "    df_comments = get_video_comments(video_id, max_results=100)\n",
    "    \n",
    "    # Only append if comments are found\n",
    "    if df_comments is not None:\n",
    "        df_comments_all = pd.concat([df_comments_all, df_comments], ignore_index=True)\n",
    "\n",
    "print(df_comments_all)\n",
    "\n",
    "\n",
    "df_comments_all.to_csv('/Users/terezasaskova/Desktop/youtubeAPI/video_comments.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/terezasaskova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Moral is don't ski or board on steep slopes wh...   \n",
      "1  Thank you for publishing this! I need advice: ...   \n",
      "2  This might be a dumb question, but how do skie...   \n",
      "3  Thanks for posting! I need advice: My Safe Wal...   \n",
      "4  Cheers for posting! Hoping for some guidance: ...   \n",
      "\n",
      "                                     cleaned_comment  \n",
      "0  moral dont ski board steep slopes u idiots fal...  \n",
      "1  thank publishing need advice crypto wallet trx...  \n",
      "2  might dumb question skiers know specific ratio...  \n",
      "3  thanks posting need advice safe wallet contain...  \n",
      "4  cheers posting hoping guidance wallet okx cont...  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_comments_all[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_comment\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     26\u001b[0m colorado_avalanches \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/terezasaskova/Downloads/colorado avalanches subreddit.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m colorado_avalanches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m colorado_avalanches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(colorado_avalanches[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m---> 16\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)  \u001b[38;5;66;03m# Remove digits\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Preprocess comments\n",
    "df_comments_all['cleaned_comment'] = df_comments_all['text'].apply(preprocess_text)\n",
    "print(df_comments_all[['text', 'cleaned_comment']].head())\n",
    "\n",
    "colorado_avalanches = pd.read_csv('/Users/terezasaskova/Downloads/colorado avalanches subreddit.csv')\n",
    "\n",
    "colorado_avalanches['cleaned_text'] = colorado_avalanches['text'].apply(preprocess_text)\n",
    "print(colorado_avalanches[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avalanche    0.028248\n",
      "snow         0.024082\n",
      "like         0.022144\n",
      "video        0.021827\n",
      "one          0.013350\n",
      "dont         0.012352\n",
      "great        0.012296\n",
      "get          0.011746\n",
      "im           0.011696\n",
      "go           0.011681\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize comments using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_comments_all['cleaned_comment'])\n",
    "\n",
    "# Convert to DataFrame to inspect top keywords\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_keywords = tfidf_df.mean(axis=0).sort_values(ascending=False).head(10)\n",
    "print(top_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "ever love please video get dont avalanche lol man thats\n",
      "Topic 2:\n",
      "time think air got dont looks bro avalanche snow like\n",
      "Topic 3:\n",
      "wtf didnt cameraman one test god oh im snow avalanche\n",
      "Topic 4:\n",
      "brad music good rip thank really helmet would great video\n",
      "Topic 5:\n",
      "amazing run wallet stupid go thank never wow clean thanks\n"
     ]
    }
   ],
   "source": [
    "#topic modeling\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # You can adjust the number of topics\n",
    "\n",
    "# Fit the LDA model on the TF-IDF matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_words = 10\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i + 1}:\")\n",
    "    top_words = [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-n_words:]]\n",
    "    print(\" \".join(top_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Vectorize comments using TF-IDF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(colorado_avalanches[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert to DataFrame to inspect top keywords\u001b[39;00m\n\u001b[1;32m      9\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cleaned_text'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Vectorize comments using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(colorado_avalanches['cleaned_text'])\n",
    "\n",
    "# Convert to DataFrame to inspect top keywords\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "top_keywords = tfidf_df.mean(axis=0).sort_values(ascending=False).head(10)\n",
    "print(top_keywords)\n",
    "\n",
    "#topic modeling\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # You can adjust the number of topics\n",
    "\n",
    "# Fit the LDA model on the TF-IDF matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "n_words = 10\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i + 1}:\")\n",
    "    top_words = [vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-n_words:]]\n",
    "    print(\" \".join(top_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
